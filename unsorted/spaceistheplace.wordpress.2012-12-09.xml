<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>

	<atom:link rel="search" type="application/opensearchdescription+xml" href="http://fredsix.wordpress.com/osd.xml" title="space is the place" />
	<atom:link rel='hub' href='http://fredsix.wordpress.com/?pushpress=hub'/>

	<item>
		<title>Models and the natural transformations between them</title>
		<link>http://fredsix.wordpress.com/?p=49</link>
		<pubDate>Wed, 30 Nov -0001 00:00:00 +0000</pubDate>
		<dc:creator>fredsix</dc:creator>
		<guid isPermaLink="false">http://fredsix.wordpress.com/?p=49</guid>
		<description></description>
		<content:encoded><![CDATA[This post is based on sections 4.1 &amp; 4.2 of Barr and Wells, <em>Category theory for computing science</em>
Some preliminary definitions:

<strong>Quiver </strong>- Directed multigraph. See <a href="http://ncatlab.org/nlab/show/quiver">http://ncatlab.org/nlab/show/quiver</a> . Barr&amp;Wells call this a "graph"

<strong>Diagram </strong>- A quiver homomorphism.

<strong>Commuting diagram </strong>- A quiver homomorphism (diagram) where the target quiver is the underlying graph of a category and where for any two points in the source graph, any two different paths between them get mapped (via the diagram) to two paths in the underlying graph that commute in the category.

<strong>Model</strong> - A diagram (not necessarily commutative) from a quiver G to (the underlying graph of) <strong>Set</strong>. Barr &amp; Wells invent, as an example of a model, a "u-structure", which is a set equipped with a unary function on the S. Then a diagram from a graph with one node and a self-arrow to the category <strong>Set</strong> is a "u-structure", so  the diagram is called a <strong>model</strong> of a u-structure. They later broaden this definition from <strong>Set</strong> to any category.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>49</wp:post_id>
		<wp:post_date>2012-10-21 22:00:49</wp:post_date>
		<wp:post_date_gmt>0000-00-00 00:00:00</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name></wp:post_name>
		<wp:status>draft</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>superawesome</wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[25949232]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Vector spaces and linear independence</title>
		<link>http://fredsix.wordpress.com/2012/12/06/vector-spaces-and-linear-independence/</link>
		<pubDate>Thu, 06 Dec 2012 04:38:55 +0000</pubDate>
		<dc:creator>fredsix</dc:creator>
		<guid isPermaLink="false">http://fredsix.wordpress.com/?p=159</guid>
		<description></description>
		<content:encoded><![CDATA[If you go back to my earlier post in November 2012  (which I am too lazy to link to), you will find that I talked about fields already. So I will briefly restate:

A <strong>field </strong>is a non-trivial commutative ring where every non-zero element has a multiplicative inverse.

A <strong>vector space </strong>is a triplet $latex (F, V, \ast: F \times V \rightarrow V)$ where $latex F$ is a field, $latex V$ is an abelian group, $latex (V, \ast$ is a <a href="http://en.wikipedia.org/wiki/Group_with_operators">group with operators</a> (i.e. currying $latex \ast$ with each $latex f \in F$ yields a group endomorphism on $latex V$) and where the following three postulates hold:
<ol>
	<li>$latex (a + b) \ast v = a \ast v + b \ast v$</li>
	<li>$latex (ab) \ast v = a \ast (b \ast v)$</li>
	<li>$latex 1 \ast v = v$</li>
</ol>
(Interestingly, the last 2 properties say that $latex \ast$ is a monoid action of $latex F$ on $latex V$ (since the multiplication operation is a monoid in a field).)

The elements of the field $latex F$ are called <strong>scalars</strong>.

If you take some set $latex x_1, \ldots, x_n$ of vectors and some corresponding set $latex a_1, \ldots, a_n$ of scalars, and form the sum of products $latex a_1 x_1 + \cdots + a_n x_n$, this result is called a <strong>linear combination</strong> of the vectors.

Now, a set of vectors $latex \left\{{x_i}\right\}$ is <strong>linearly dependent </strong>if there exists a set $latex \left\{{a_i}\right\}$ of scalars for which one of the scalars $latex a_k$ is nonzero and $latex \Sigma_i a_i x_i = 0$. A set is <strong>linearly independent</strong> if $latex \Sigma_i a_i x_i = 0$ implies $latex a_i = 0$ for all $latex i$. That is, linearly independent means that the only way for a linear combination of the vectors to result in the zero vector is for all the scalars to be zero.

(I'm going off of Halmos for this, and he does something a bit strange by defining linear independence vacuously as well. So the empty set is linearly independent vacuously because it cannot possibly be linearly dependent (there is no non-zero scalar to be found)).

This is a pretty abstract definition (why do we care about how to get zero from linear combinations?), but the importance is revealed by the following lemma:

<strong>Lemma:</strong> If the set of vectors $latex \left\{{x_i}\right\}$ is linearly independent then $latex x$ is a linear combination of the $latex \left\{{x_i}\right\}$ iff $latex \left\{{x}\right\} \cup \left\{{x_i}\right\}$ is linearly dependent.

<em>Proof: </em>Supposing the $latex x_i$'s are linearly independent, then if $latex x$ is a linear combination of them, then for some set of $latex b_i$'s, $latex x = \Sigma_i b_i x_i$. So we can take $latex b = -1$ and get $latex b x + \Sigma_i b_i x_i = 0$ and at least one of the scalars is nonzero, proving that adding $latex x$ to the $latex x_i$'s makes the new set linearly dependent.

For the converse, assume that the $latex x$ plus $latex x_i$'s are linearly dependent. So either $latex b \neq 0$ or some $latex b_k \neq 0$ for $latex b x + \Sigma_i b_i x_i = 0$. If $latex b \neq 0$ then we can divide the whole equation out by $latex b$ and subtract the linear combination of $latex x_i$'s to the other side, which expresses $latex x$ as a linear combination of the $latex x_i$'s (which is what we're trying to find). So suppose that $latex b = 0$ and that some $latex b_k \neq 0$ instead. We have:

$latex bx + \Sigma_i b_i x_i = 0 + \Sigma_i b_i x_i =\Sigma_i b_i x_i = 0$

And $latex b_k \neq 0$, so the $latex \left\{{x_i}\right\}$ is linearly dependent, contrary to our first assumption. So $latex b \neq 0$ and $latex x$ is a linear combination of the $latex x_i$'s. QED.

Actually, we didn't need that lemma to see the significance of the "linearly dependent" definition, because the key idea is that, with a linearly dependent set, some linear combination of the vectors with one of the scalars nonzero, call it $latex b$, results in the zero vector, so we can divide the equation by $latex -b$ to express one of the vectors as a linear combination of the others. That's really the main point: we could throw away one of the vectors and still not miss it (due to being able to reconstruct it as a linear combination of the others).

To round out this post I will finish with what Halmos calls "the  fundamental result concerning linear dependence":

<strong>Theorem: </strong>The n-tuple $latex (x_1, \ldots, x_n)$ is linearly dependent iff there's some $latex k$, $latex 2 \leq k \leq n$ for which $latex x_k$ is a linear combination of $latex \left\{{x_1, \ldots, x_{k-1}}\right\}$.

<em>Proof: </em>$latex (\Rightarrow)$ If $latex (x_1, \ldots, x_n)$ is linearly dependent, then pick the smallest $latex k$ such that $latex x_1, \ldots, x_k$  is linearly dependent (we could, at the very least, select $latex k=n$. Since the set of all $latex j$ such that $latex x_1, \ldots, x_j$ is linearly dependent is nonempty, by the well-ordering principle it has a smallest element.) $latex \left\{{x_1, \ldots, x_{k-1}}\right\}$ must be linearly independent, for if it were otherwise, it would contradict $latex k$ being smallest. In addition, adding $latex x_k$ to it makes it linearly dependent, so $latex x_k$ must be a linear combination of the preceding elements by the lemma above.

$latex (\Leftarrow)$ Clearly the converse holds. If one of the $latex x_k$ is a linear combination of the preceding vectors, then we can obtain zero with $latex \Sigma_{i=1}^{k} (a_i x_i) - x + \Sigma_{i=k+1}^{n} 0 x_i$.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>159</wp:post_id>
		<wp:post_date>2012-12-06 04:38:55</wp:post_date>
		<wp:post_date_gmt>2012-12-06 04:38:55</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>vector-spaces-and-linear-independence</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[25949232]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>superawesome</wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_pending</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>jabber_published</wp:meta_key>
			<wp:meta_value><![CDATA[1354768737]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Basii</title>
		<link>http://fredsix.wordpress.com/2012/12/09/basii/</link>
		<pubDate>Sun, 09 Dec 2012 04:41:46 +0000</pubDate>
		<dc:creator>fredsix</dc:creator>
		<guid isPermaLink="false">http://fredsix.wordpress.com/?p=183</guid>
		<description></description>
		<content:encoded><![CDATA[This post is about bases of vector spaces and a crucial property of them: that every linearly independent set of vectors can be extended to a basis (and, thus, that every vector space has a basis). There are two sides to this, actually: there's an easy proof that's restricted to finite dimensional vector spaces, and then there's the general proof for arbitrary vector spaces that uses Zorn's Lemma. I will cover both, because the infinite case is not too hard and rather fun.

<strong>Definition: </strong>The <strong>span</strong> of a set of vectors is the set of all linear combinations of the vectors.

<strong>Definition: </strong>A <strong>basis</strong> for a vector space $latex V$ is a linearly independent set $latex B$ for which $latex span B = V$.

<strong>Definition: </strong>A vector space is <strong>finite-dimensional </strong>if a finite basis exists for it.

<strong>Lemma/Theorem/whatever: </strong>A given basis  $latex B$ determines unique set of coefficients for each $lavex v \in V$ such that $latex v = \Sigma_i \alpha_i b_i$.

<em>Proof: </em>if $latex v = \Sigma_i \alpha_i b_i = \Sigma_i \xi_i b_i$, then $latex 0 = \Sigma_i (\alpha_i - \xi_i) b_i$. Since $latex B$ is a basis, it's linearly independent, so each $latex \alpha_i - \xi_i = 0$, aka $latex \alpha_i = \xi_i$. QED

Another way of looking (at this is as saying that the function defined from basis set (bases n-tuple, really) to n-tuple of linear combination coefficients is well-defined.

<strong>Theorem: </strong>In a finite-dimensional vector space $latex V$, any linearly independent set $latex A = \left\{{y_1, \ldots, y_k}\right\}$ can be extended to a basis $latex \left\{{y_1, \ldots, y_k, y_{k+1}, \ldots, y_n}\right\} for $latex V$.

<em>Fake Proof:</em>  If $latex A$ is a basis, we're done. Otherwise, some vector $latex v_1 \in V$ is not a linear combination of $latex A$, so set $latex A_1 = A \cup \left\{{v_1}\right\}$. Now repeat; if $latex A_1$ is a basis, we're done, otherwise we can find a $latex v_2$ which is not a linear combination of $latex A_1$. Since $latex V$ is finite dimensional, this construction will eventually find a basis. QED.

Why does the fake proof not work? It's actually fine up to the last line, and it would work if we knew that every basis for a vector space was the same size. (To expand on this: "finite-dimensional" at this point simply means that there's some set for which every vector in $latex V$ is a linear combination of the set. We know there's one such set, but we have no information about what happens when we string together another collection of linearly independent vectors of the same size as the basis.) Since we have not yet proved this, we must resort to a different proof:

<em>Proof:  </em>Since our previous strategy of adding vectors failed because we couldn't guarantee that it would terminate, we will try the opposite strategy: we have a basis $latex \left\{{x_1, \ldots, x_n}\right\}$, so let's add <em>too many</em> vectors to begin, and take them out until the set is linearly indepenent again.

So we take our linearly independent set $latex \left\{{y_1, \ldots, y_k}\right\}$ and the basis and smash them together into a tuple: $latex \left\{{y_1, \ldots, y_k, x_1, \ldots, x_n}\right\}$. This set of vectors is linearly dependent (each $latex y$ is a linear combination of the basis elements), so by the theorem we proved in the last post, there's one element which is a linear combination of the preceding elements. Furthermore, it can't be one of the $latex y_i$'s, because those are linearly independent. So there's some $latex j$ such that $latex x_j$ is in the span of $latex A \cup \left\{{x_1, \ldots, x_{j-1}}\right\}$.

The idea is to just remove that $latex x_j$, resulting in $latex y_1, \ldots, y_k, x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_n$. This set still spans $latex V$ because any vector requiring a nonzero coefficient of $latex x_j$ can just use the linear combination to substitute for $latex x_j$. Keep removing basis elements until the set is no longer linearly dependent. At each step, the span never changes, so the result is a basis. QED.

That was instructive, but let's go for the general case. Halmos indicates that this requires "transfinite trickery", but I'm not so certain about that..

<strong>Theorem: </strong>Every vector space has a basis.

<em>Proof: </em>We need an initial result -- the Kuratowski/Zorn Lemma -- which says:

<em>Kuratowski/Zorn Lemma: </em>For any poset $latex P$, if every chain (totally ordered subset) is bounded above, then $latex P$ has a maximal element.

Now take the set $latex \mathcal{L}$ of all linearly independent subsets of $latex V$. This is a poset by set inclusion. For any chain $latex \mathcal{C}$ in $latex \mathcal{L}$, the union $latex \bigcup \mathcal{C}$ is in $latex \mathcal{L}$ because, for any finite subset $latex A$ of $latex \bigcup \mathcal{C}$, there's a linearly independent set $latex C$ for each $latex a \in A$ that (1) contains $latex a$ and (2) is in $latex \mathcal{C}$. Since it's a chain, the biggest of these chains actually contains all elements of $latex A$. So every finite subset of $latex \bigcup \mathcal{C}$ is linearly independent, hence the union is independent. So the union is clearly an upper bound in $latex \mathcal{L}$. So it satisfies the premises for K/L, and hence $latex \mathcal{L}$ has a maximal element $latex M$. This is linearly independent (it's in $latex \mathcal{L}$), so we simply need to prove that it spans $latex V$.

If $latex x \notin M$, we know that $latex X = M \cup \left\{{x}\right\}$ can't be linearly independent, because $latex M$ is maximal. So $latex X$ is linearly dependent, meaning that some finite subset $latex U$ is linearly dependent. $latex U$ must contain $latex x$, otherwise $latex U$ would be a subset of $latex M$, making $latex M$ not linearly independent. Furthermore $latex U - \left\{{x}\right\}$ is linearly independent, so by a theorem in the previous post, $latex x$ must be a linear combination of the $latex U - \left\{{x}\right\}$. Thus $latex x$ is in $latex span M$ after all. QED.

The Kuratowski/Zorn Lemma is equivalent to the Axiom of Choice. That's the price of great power.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>183</wp:post_id>
		<wp:post_date>2012-12-09 04:41:46</wp:post_date>
		<wp:post_date_gmt>2012-12-09 04:41:46</wp:post_date_gmt>
		<wp:comment_status>open</wp:comment_status>
		<wp:ping_status>open</wp:ping_status>
		<wp:post_name>basii</wp:post_name>
		<wp:status>publish</wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type>post</wp:post_type>
		<wp:post_password></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key>_edit_last</wp:meta_key>
			<wp:meta_value><![CDATA[25949232]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>superawesome</wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>_publicize_pending</wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key>jabber_published</wp:meta_key>
			<wp:meta_value><![CDATA[1355028106]]></wp:meta_value>
		</wp:postmeta>
	</item>
</channel>
</rss>
